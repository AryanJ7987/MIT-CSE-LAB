{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b746eeb-1f80-4b92-b71f-dfcad6a67a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical Gradient (dz/da): 64.0\n",
      "Gradient from Backpropagation (dz/da): 64.0\n"
     ]
    }
   ],
   "source": [
    "#question 1\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Define the variables\n",
    "a = Variable(torch.tensor([2.0]), requires_grad=True)\n",
    "b = Variable(torch.tensor([3.0]), requires_grad=True)\n",
    "\n",
    "# Define the computation graph\n",
    "x = 2 * a + 3 * b\n",
    "y = 5 * a**2 + 3 * b**3\n",
    "z = 2 * x + 3 * y\n",
    "\n",
    "# Analytical gradient\n",
    "dz_da_analytical = 4 + 30 * a\n",
    "\n",
    "# Backward pass to compute gradient using automatic differentiation\n",
    "z.backward()\n",
    "\n",
    "# Access the gradient obtained by backpropagation\n",
    "dz_da_backpropagation = a.grad\n",
    "\n",
    "# Compare the results\n",
    "print(\"Analytical Gradient (dz/da):\", dz_da_analytical.item())\n",
    "print(\"Gradient from Backpropagation (dz/da):\", dz_da_backpropagation.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aff6c038-0e3f-4c18-99fc-2f7fcdbdb3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient from Backpropagation (da/dw): 2.0\n"
     ]
    }
   ],
   "source": [
    "#question 2\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "def relu(x):\n",
    "    return max(0.0, x)\n",
    "    \n",
    "b = Variable(torch.tensor([1.0]), requires_grad=True)\n",
    "x = Variable(torch.tensor([2.0]), requires_grad=True)\n",
    "w = Variable(torch.tensor([3.0]), requires_grad=True)\n",
    "\n",
    "u = w*x\n",
    "v = u+b\n",
    "a = relu(v)\n",
    "# Backward pass to compute gradient using automatic differentiation\n",
    "a.backward()\n",
    "\n",
    "# Access the gradient obtained by backpropagation\n",
    "da_dw_backpropagation = w.grad\n",
    "\n",
    "#print(\"Analytical Gradient (da/dw):\", da_dw_analytical.item())\n",
    "print(\"Gradient from Backpropagation (da/dw):\", da_dw_backpropagation.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d0b49e7-b8bb-4e6b-802c-ba0669618ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient from Backpropagation (da/dw): 0.001820442616008222\n"
     ]
    }
   ],
   "source": [
    "#question 3\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "    \n",
    "b = Variable(torch.tensor([1.0]), requires_grad=True)\n",
    "x = Variable(torch.tensor([2.0]), requires_grad=True)\n",
    "w = Variable(torch.tensor([3.0]), requires_grad=True)\n",
    "\n",
    "u = w*x\n",
    "v = u+b\n",
    "a = sigmoid(v)\n",
    "# Backward pass to compute gradient using automatic differentiation\n",
    "a.backward()\n",
    "\n",
    "# Access the gradient obtained by backpropagation\n",
    "da_dw_backpropagation = w.grad\n",
    "\n",
    "#print(\"Analytical Gradient (da/dw):\", da_dw_analytical.item())\n",
    "print(\"Gradient from Backpropagation (da/dw):\", da_dw_backpropagation.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed6b5a6b-12a3-4465-8347-eee7317a3361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical Gradient (df/dx): -0.0007545282132923603\n",
      "Gradient from Backpropagation (df/dx): -0.0007545282132923603\n"
     ]
    }
   ],
   "source": [
    "#question 4\n",
    "import torch\n",
    "\n",
    "# Define the variable\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Define the function\n",
    "f = torch.exp(-x**2 - 2*x - torch.sin(x))\n",
    "\n",
    "# Manually compute the analytical gradient\n",
    "df_dx_analytical = f * (-2*x - 2 - torch.cos(x))\n",
    "\n",
    "# Backward pass to compute gradient using automatic differentiation\n",
    "f.backward()\n",
    "\n",
    "# Access the gradient obtained by backpropagation\n",
    "df_dx_backpropagation = x.grad\n",
    "\n",
    "# Compare the results\n",
    "print(\"Analytical Gradient (df/dx):\", df_dx_analytical.item())\n",
    "print(\"Gradient from Backpropagation (df/dx):\", df_dx_backpropagation.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45210d39-428a-4b25-a44c-6834608e1c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient by backpropagation(dy/dx): tensor([61.])\n"
     ]
    }
   ],
   "source": [
    "#question 5\n",
    "\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = 8*x**4 + 3*x**3 + 7*x**2 + 6*x + 3\n",
    "y.backward()\n",
    "dy_dx_backp = x.grad\n",
    "print(\"gradient by backpropagation(dy/dx):\",dy_dx_backp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa6374c-be98-4d64-9cd8-01ca822d7347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
