{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------------------------+\n",
      "|    item|price|price after applying GST|\n",
      "+--------+-----+------------------------+\n",
      "|notebook|   30|                    52.5|\n",
      "|   paint|   35|                    57.5|\n",
      "+--------+-----+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#QUESTION 1\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a SparkSession object\n",
    "spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [(\"pencil\", 10), (\"notebook\", 30), (\"paint\", 35)]\n",
    "df = spark.createDataFrame(data, [\"item\", \"price\"])\n",
    "\n",
    "# Apply filter transformation\n",
    "df_filtered = df.filter(col(\"price\") > 25)\n",
    "\n",
    "# Apply withColumn transformation\n",
    "df_with_column = df_filtered.withColumn(\"price after applying GST\", col(\"price\") + 22.5)\n",
    "\n",
    "# Display the final DataFrame\n",
    "df_with_column.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows in the DataFrame is 4.\n",
      "+--------+-----+\n",
      "|    item|price|\n",
      "+--------+-----+\n",
      "|  pencil|   10|\n",
      "|notebook|   30|\n",
      "|   paint|   35|\n",
      "| fevicol|   28|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 2\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName('PySpark').getOrCreate()\n",
    "\n",
    "# Create DataFrame\n",
    "data = [(\"pencil\", 10), (\"notebook\", 30), (\"paint\", 35), (\"fevicol\", 28)]\n",
    "columns = [\"item\", \"price\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Count rows in DataFrame\n",
    "rows = df.count()\n",
    "print(f\"The number of rows in the DataFrame is {rows}.\")\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|   Name|Department|Salary|\n",
      "+-------+----------+------+\n",
      "|  Alice|     Sales|  3000|\n",
      "|    Bob|        IT|  4000|\n",
      "|Charlie|     Sales|  3500|\n",
      "|  David|        IT|  4500|\n",
      "+-------+----------+------+\n",
      "\n",
      "Total salary by department:\n",
      "+----------+-----------+\n",
      "|Department|sum(Salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|       6500|\n",
      "|        IT|       8500|\n",
      "+----------+-----------+\n",
      "\n",
      "Average salary by department:\n",
      "+----------+-----------+\n",
      "|Department|avg(Salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|     3250.0|\n",
      "|        IT|     4250.0|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#QUESTION 3\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, avg\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName('PySpark').getOrCreate()\n",
    "\n",
    "# Create DataFrame\n",
    "data = [(\"Alice\", \"Sales\", 3000), (\"Bob\", \"IT\", 4000), (\"Charlie\", \"Sales\", 3500), (\"David\", \"IT\", 4500)]\n",
    "columns = [\"Name\", \"Department\", \"Salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Total salary by department\n",
    "total_salary = df.groupBy(\"Department\").agg(sum(\"Salary\"))\n",
    "\n",
    "# Display the result\n",
    "print(\"Total salary by department:\")\n",
    "total_salary.show()\n",
    "\n",
    "# Average salary by department\n",
    "average_salary = df.groupBy(\"Department\").agg(avg(\"Salary\"))\n",
    "\n",
    "# Display the result\n",
    "print(\"Average salary by department:\")\n",
    "average_salary.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTION 4\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName('PySpark').getOrCreate()\n",
    "\n",
    "# Create DataFrame\n",
    "data = [(\"Alice\", \"Sales\", 3000), (\"Bob\", \"IT\", 4000), (\"Charlie\", \"Sales\", 3500), (\"David\", \"IT\", 4500)]\n",
    "columns = [\"Name\", \"Department\", \"Salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Write DataFrame to CSV file\n",
    "df.write.csv(\"/home/lplab/Desktop/210962018/q4.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#question 1,2,4 combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n",
      "|    Name|Department|Salary|\n",
      "+--------+----------+------+\n",
      "|   Aryan|   backend| 75000|\n",
      "|   rohit|  frontend| 47000|\n",
      "|yashveer|  frontend| 55000|\n",
      "|   rehan|   backend| 65000|\n",
      "+--------+----------+------+\n",
      "\n",
      "+--------+----------+------+------------------------+\n",
      "|    Name|Department|Salary|Salary after cutting GST|\n",
      "+--------+----------+------+------------------------+\n",
      "|   Aryan|   backend| 75000|                69299.72|\n",
      "|yashveer|  frontend| 55000|                49299.72|\n",
      "|   rehan|   backend| 65000|                59299.72|\n",
      "+--------+----------+------+------------------------+\n",
      "\n",
      "The number of rows in the DataFrame is 4.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName('PySpark').getOrCreate()\n",
    "\n",
    "# Example: DataFrame with filter and withColumn transformations\n",
    "data = [(\"Aryan\", \"backend\", 75000), (\"rohit\", \"frontend\", 47000), (\"yashveer\", \"frontend\", 55000), (\"rehan\", \"backend\", 65000)]\n",
    "columns = [\"Name\", \"Department\", \"Salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "# Apply filter transformation\n",
    "df_filtered = df.filter(col(\"Salary\") > 50000)  # Corrected missing parenthesis\n",
    "\n",
    "# Apply withColumn transformation\n",
    "df_with_column = df_filtered.withColumn(\"Salary after cutting GST\", col(\"Salary\") - 5700.28)\n",
    "df_with_column.show()\n",
    "# Count rows in DataFrame\n",
    "rows = df.count()\n",
    "print(f\"The number of rows in the DataFrame is {rows}.\")\n",
    "\n",
    "# Specify the output CSV file path\n",
    "output_path = \"/home/lplab/Desktop/210962018/output.csv\"\n",
    "\n",
    "# Write DataFrame to CSV\n",
    "df.write.csv(output_path, header=True, mode=\"overwrite\")  # Corrected df3 to df\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count:\n",
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|beach|    4|\n",
      "|malpe|    1|\n",
      "|mattu|    1|\n",
      "| baga|    1|\n",
      "|hoode|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#QUESTION 5\n",
    "# QUESTION 5\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName('WordCountExample').getOrCreate()\n",
    "\n",
    "# Sample text data\n",
    "text_data = [\"malpe beach\",\"mattu beach\",\"baga beach\", \"hoode beach\"]\n",
    "\n",
    "# Create a DataFrame from the text data\n",
    "df = spark.createDataFrame([(line,) for line in text_data], [\"text\"])\n",
    "\n",
    "# Split the text into words using space as a delimiter and explode the array of words\n",
    "word_count = df.select(explode(split(\"text\", \" \")).alias(\"word\")).groupBy(\"word\").count()\n",
    "\n",
    "# Display the result\n",
    "print(\"Word Count:\")\n",
    "word_count.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
